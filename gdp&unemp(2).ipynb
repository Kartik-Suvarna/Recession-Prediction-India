{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xt8s2z7huG3A",
      "metadata": {
        "id": "Xt8s2z7huG3A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "gdp=pd.read_excel(\"/content/data2.xls\")\n",
        "unemp=pd.read_excel(\"/content/data.xlsx\")\n",
        "unemp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabb7c0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eabb7c0f",
        "outputId": "7d9a0cd3-f35a-4320-e5fa-0e1ce3752486"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gdp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d099fb13053f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extracting the GDP values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgdp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GDP change'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Normalize GDP data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Allow for negative values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgdp_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdp_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gdp' is not defined"
          ]
        }
      ],
      "source": [
        "# Extracting the GDP values\n",
        "gdp_values = gdp['GDP change'].values.reshape(-1, 1)\n",
        "# Normalize GDP data\n",
        "scaler = MinMaxScaler(feature_range=(-5,3))  # Allow for negative values\n",
        "gdp_scaled = scaler.fit_transform(gdp_values)\n",
        "\n",
        "# Define the number of previous years to consider for prediction\n",
        "window_size = 10\n",
        "\n",
        "# Create sequences and labels\n",
        "sequences = []\n",
        "labels = []\n",
        "for i in range(len(gdp_scaled) - window_size):\n",
        "    sequences.append(gdp_scaled[i:i + window_size])\n",
        "    labels.append(gdp_scaled[i + window_size])\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape sequences for CNN input (samples, window_size, 1)\n",
        "X_train = X_train.reshape(X_train.shape[0], window_size, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], window_size, 1)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, input_shape=(window_size, 1)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=64, kernel_size=3),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(units=128),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dense(units=64),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dense(units=1, activation='linear')  # Use linear activation for flexibility\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# Predict GDP for the next 10 years\n",
        "last_sequence = sequences[-1].reshape(1, window_size, 1)\n",
        "future_gdp_scaled = []\n",
        "\n",
        "no_of_years_to_predict=30\n",
        "# Use the model to predict GDP\n",
        "for i in range(no_of_years_to_predict):\n",
        "    prediction = model.predict(last_sequence)\n",
        "    future_gdp_scaled.append(prediction)\n",
        "    last_sequence = np.concatenate([last_sequence[:, 1:, :], prediction.reshape(1, 1, 1)], axis=1)\n",
        "\n",
        "future_gdp_scaled = np.array(future_gdp_scaled).reshape(-1)\n",
        "predicted_gdp = scaler.inverse_transform(future_gdp_scaled.reshape(-1, 1))\n",
        "\n",
        "\n",
        "# Plotting the results using Plotly\n",
        "years = gdp['Year']\n",
        "actual_trace = go.Scatter(x=years, y=gdp_values.flatten(), mode='lines', name='Actual GDP')\n",
        "predicted_trace = go.Scatter(x=list(range(max(years), max(years) + no_of_years_to_predict)), y=predicted_gdp.flatten(), mode='lines', name='Predicted GDP')\n",
        "\n",
        "layout = go.Layout(title=f'Predicted GDP of a Country for the Next {no_of_years_to_predict}  Years',\n",
        "                   xaxis=dict(title='Year'),\n",
        "                   yaxis=dict(title='GDP'))\n",
        "\n",
        "fig = go.Figure(data=[actual_trace, predicted_trace], layout=layout)\n",
        "fig.show()\n",
        "predicted_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0KJFR813rxr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q0KJFR813rxr",
        "outputId": "5fbec4aa-8918-451a-aefd-f33b6902f781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5914 - val_loss: 4.0590\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.1823 - val_loss: 4.4156\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.1492 - val_loss: 3.9662\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8969 - val_loss: 3.4555\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7118 - val_loss: 3.1383\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6420 - val_loss: 3.0065\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.5919 - val_loss: 3.0102\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4954 - val_loss: 3.1068\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3768 - val_loss: 3.2677\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2924 - val_loss: 3.4166\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2583 - val_loss: 3.4384\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2278 - val_loss: 3.2789\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1664 - val_loss: 3.0161\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.1059 - val_loss: 2.7674\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0867 - val_loss: 2.6103\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0879 - val_loss: 2.5650\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0653 - val_loss: 2.6230\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0362 - val_loss: 2.7412\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0396 - val_loss: 2.8203\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0582 - val_loss: 2.7887\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0543 - val_loss: 2.6940\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0432 - val_loss: 2.6307\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0486 - val_loss: 2.6500\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0540 - val_loss: 2.7533\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0419 - val_loss: 2.9144\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0272 - val_loss: 3.0819\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0238 - val_loss: 3.1927\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0227 - val_loss: 3.2172\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0148 - val_loss: 3.1811\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0073 - val_loss: 3.1314\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0078 - val_loss: 3.0990\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0118 - val_loss: 3.0946\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0123 - val_loss: 3.1122\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0102 - val_loss: 3.1347\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0106 - val_loss: 3.1383\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0132 - val_loss: 3.1059\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0135 - val_loss: 3.0391\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0107 - val_loss: 2.9591\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0081 - val_loss: 2.8902\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0075 - val_loss: 2.8496\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0071 - val_loss: 2.8418\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0051 - val_loss: 2.8599\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0029 - val_loss: 2.8894\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0024 - val_loss: 2.9119\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0029 - val_loss: 2.9146\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0028 - val_loss: 2.8989\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0020 - val_loss: 2.8774\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0020 - val_loss: 2.8655\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0028 - val_loss: 2.8721\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0032 - val_loss: 2.8970\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0026 - val_loss: 2.9303\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0022 - val_loss: 2.9575\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0022 - val_loss: 2.9673\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0020 - val_loss: 2.9578\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0012 - val_loss: 2.9373\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 6.1760e-04 - val_loss: 2.9180\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.0003e-04 - val_loss: 2.9091\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.7028e-04 - val_loss: 2.9128\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.0849e-04 - val_loss: 2.9250\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 4.4532e-04 - val_loss: 2.9374\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.0401e-04 - val_loss: 2.9423\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9.3982e-04 - val_loss: 2.9373\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.7081e-04 - val_loss: 2.9262\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7.1261e-04 - val_loss: 2.9160\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.9567e-04 - val_loss: 2.9128\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.6785e-04 - val_loss: 2.9185\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.6784e-04 - val_loss: 2.9299\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.5830e-04 - val_loss: 2.9408\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.1312e-04 - val_loss: 2.9452\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.2068e-04 - val_loss: 2.9409\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.4920e-04 - val_loss: 2.9307\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9.6010e-05 - val_loss: 2.9207\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.5200e-04 - val_loss: 2.9160\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.1130e-04 - val_loss: 2.9179\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.8824e-04 - val_loss: 2.9241\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.6463e-04 - val_loss: 2.9295\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.9538e-04 - val_loss: 2.9302\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 2.0120e-04 - val_loss: 2.9256\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.4600e-04 - val_loss: 2.9185\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0319e-04 - val_loss: 2.9133\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9.9407e-05 - val_loss: 2.9129\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.9671e-05 - val_loss: 2.9171\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3.8912e-05 - val_loss: 2.9233\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 2.8853e-05 - val_loss: 2.9278\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.7014e-05 - val_loss: 2.9285\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.9199e-05 - val_loss: 2.9259\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.1338e-05 - val_loss: 2.9226\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.2350e-05 - val_loss: 2.9213\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.5023e-05 - val_loss: 2.9232\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.4940e-05 - val_loss: 2.9270\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.0434e-05 - val_loss: 2.9301\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 3.8813e-05 - val_loss: 2.9305\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.3715e-05 - val_loss: 2.9281\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.8263e-05 - val_loss: 2.9243\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0663e-05 - val_loss: 2.9216\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.3695e-05 - val_loss: 2.9213\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.2427e-05 - val_loss: 2.9230\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8.5372e-06 - val_loss: 2.9250\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2469e-05 - val_loss: 2.9256\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.7663e-05 - val_loss: 2.9243\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1290\n",
            "Test Loss: 1.129042148590088\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"058ba68a-8449-42c4-9475-146456bafe69\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"058ba68a-8449-42c4-9475-146456bafe69\")) {                    Plotly.newPlot(                        \"058ba68a-8449-42c4-9475-146456bafe69\",                        [{\"mode\":\"lines\",\"name\":\"Actual GDP\",\"x\":[1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024],\"y\":[0.1222222,0.13988471913360656,0.031127733527470006,-0.2462570001872123,0.07006368773683272,0.036133668776516886,0.07702522787960205,0.044654594147595414,0.05530323934078706,0.03767691750743343,0.16999636678014615,0.13785044451744402,-0.03248286540882587,0.020311933083257426,0.1568964071365254,0.10543265634821153,0.08978187887999961,0.19064925994798576,0.015092418437553596,0.014147261460687626,0.06313368115501657,-0.04971637834214935,0.07157195844341996,0.04718332432493803,0.0961741165570071,0.03995208362082826,-0.023265353338294402,0.06126634728980311,-0.17599827456321324,0.04511529024025957,-0.050566857538661754,0.14834654901562327,0.07914117469060548,0.06945159391228523,0.03834393753650162,-0.0057588170724661054,0.06897322501225683,0.002434054697532666,0.017818385285429884,0.0420823188228747,0.1599664325156704,0.14758152186774884,0.13844503301654193,0.1287870425205419,0.27520566474283686,-0.02857938184532083,0.10380712350840698,0.23161587117351737,0.07327580718416,-0.010751433850384252,0.002816574155007403,0.08470232652564433,0.01943153780994354,0.07804503165508585,0.14215317323711368,0.008380066889213122,0.038384787171204054,-0.06679664668745,0.16982231752885643,0.07718993070501819,0.08360486975931579,0.04529235009282477],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Predicted GDP\",\"x\":[2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073],\"y\":[0.06187959760427475,0.042426273226737976,0.009961036033928394,0.02819804660975933,0.007544798776507378,0.004014667589217424,0.0029400838539004326,0.0980830118060112,0.0902324840426445,0.1520119309425354,0.1269833892583847,0.0495886504650116,0.04876056686043739,0.020477712154388428,0.07252652943134308,0.047989387065172195,0.08509519696235657,0.0733325183391571,0.08773082494735718,0.0864817202091217,0.05998041108250618,0.04351265728473663,0.04656533896923065,0.02110050991177559,0.10684947669506073,0.08385110646486282,0.0979524627327919,0.10114417970180511,0.08024439215660095,0.0505685918033123,0.04051027074456215,0.05122150480747223,0.047292158007621765,0.06847464293241501,0.08756843209266663,0.08774644136428833,0.10358402878046036,0.09389160573482513,0.06498967856168747,0.05533495172858238,0.0444067120552063,0.07795163989067078,0.07681230455636978,0.08891191333532333,0.08352366834878922,0.0872897207736969,0.07442352920770645,0.058072712272405624,0.0588231161236763,0.058755919337272644],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Predicted GDP of a Country for the Next 10 Years\"},\"xaxis\":{\"title\":{\"text\":\"Year\"}},\"yaxis\":{\"title\":{\"text\":\"GDP\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('058ba68a-8449-42c4-9475-146456bafe69');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Extracting the GDP values\n",
        "gdp_values = gdp['GDP change'].values.reshape(-1, 1)\n",
        "# Normalize GDP data\n",
        "scaler = MinMaxScaler(feature_range=(-5, 3))  # Allow for negative values\n",
        "gdp_scaled = scaler.fit_transform(gdp_values)\n",
        "\n",
        "# Define the number of previous years to consider for prediction\n",
        "window_size = 30\n",
        "\n",
        "# Create sequences and labels\n",
        "sequences = []\n",
        "labels = []\n",
        "for i in range(len(gdp_scaled) - window_size):\n",
        "    sequences.append(gdp_scaled[i:i + window_size])\n",
        "    labels.append(gdp_scaled[i + window_size])\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.3, random_state=72)\n",
        "\n",
        "# Reshape sequences for CNN input (samples, window_size, 1)\n",
        "X_train = X_train.reshape(X_train.shape[0], window_size, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], window_size, 1)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, input_shape=(window_size, 1)),\n",
        "    LeakyReLU(alpha=0.5),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=64, kernel_size=3),\n",
        "    LeakyReLU(alpha=0.8),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(units=128),\n",
        "    LeakyReLU(alpha=0.5),\n",
        "    Dense(units=64),\n",
        "    LeakyReLU(alpha=0.8),\n",
        "    Dense(units=1, activation='linear')  # Use linear activation for flexibility\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# Predict GDP for the next 10 years\n",
        "last_sequence = sequences[-1].reshape(1, window_size, 1)\n",
        "future_gdp_scaled = []\n",
        "\n",
        "# Use the model to predict GDP\n",
        "for i in range(50):\n",
        "    prediction = model.predict(last_sequence)\n",
        "    future_gdp_scaled.append(prediction)\n",
        "    last_sequence = np.concatenate([last_sequence[:, 1:, :], prediction.reshape(1, 1, 1)], axis=1)\n",
        "\n",
        "future_gdp_scaled = np.array(future_gdp_scaled).reshape(-1)\n",
        "predicted_gdp = scaler.inverse_transform(future_gdp_scaled.reshape(-1, 1))\n",
        "\n",
        "# Plotting the results using Plotly\n",
        "years = gdp['Year']\n",
        "actual_trace = go.Scatter(x=years, y=gdp_values.flatten(), mode='lines', name='Actual GDP')\n",
        "predicted_trace = go.Scatter(x=list(range(max(years), max(years) + 50)), y=predicted_gdp.flatten(), mode='lines', name='Predicted GDP')\n",
        "\n",
        "layout = go.Layout(title='Predicted GDP of a Country for the Next 10 Years',\n",
        "                   xaxis=dict(title='Year'),\n",
        "                   yaxis=dict(title='GDP'))\n",
        "\n",
        "fig = go.Figure(data=[actual_trace, predicted_trace], layout=layout)\n",
        "predicted_trace\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L6QAj3JIBdcH",
      "metadata": {
        "id": "L6QAj3JIBdcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "023ca82f-6a9e-4e74-c399-143b595c76bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2710 - val_loss: 0.2203\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2117 - val_loss: 0.1759\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.1662 - val_loss: 0.1350\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1259 - val_loss: 0.0989\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0918 - val_loss: 0.0656\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0632 - val_loss: 0.0388\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0437 - val_loss: 0.0225\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0371 - val_loss: 0.0189\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0441 - val_loss: 0.0233\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0568 - val_loss: 0.0267\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0634 - val_loss: 0.0258\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0607 - val_loss: 0.0224\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0522 - val_loss: 0.0194\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0428 - val_loss: 0.0189\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0358 - val_loss: 0.0211\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0323 - val_loss: 0.0253\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0319 - val_loss: 0.0302\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0333 - val_loss: 0.0344\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0351 - val_loss: 0.0371\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0363 - val_loss: 0.0381\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0364 - val_loss: 0.0377\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0355 - val_loss: 0.0356\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0335 - val_loss: 0.0325\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0309 - val_loss: 0.0289\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0280 - val_loss: 0.0255\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0255 - val_loss: 0.0226\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0236 - val_loss: 0.0204\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0225 - val_loss: 0.0191\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0221 - val_loss: 0.0186\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0222 - val_loss: 0.0186\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0223 - val_loss: 0.0186\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0221 - val_loss: 0.0186\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0216 - val_loss: 0.0186\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0208 - val_loss: 0.0187\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0202 - val_loss: 0.0192\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0201 - val_loss: 0.0200\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0204 - val_loss: 0.0206\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0207 - val_loss: 0.0209\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0207 - val_loss: 0.0207\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0203 - val_loss: 0.0202\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0199 - val_loss: 0.0197\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0195 - val_loss: 0.0193\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0193 - val_loss: 0.0190\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0193 - val_loss: 0.0189\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0193 - val_loss: 0.0189\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0192 - val_loss: 0.0190\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0190 - val_loss: 0.0193\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0187 - val_loss: 0.0197\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0185 - val_loss: 0.0203\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0184 - val_loss: 0.0207\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0184 - val_loss: 0.0210\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0183 - val_loss: 0.0209\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0182 - val_loss: 0.0206\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0180 - val_loss: 0.0202\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0178 - val_loss: 0.0197\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0176 - val_loss: 0.0193\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0175 - val_loss: 0.0191\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0174 - val_loss: 0.0191\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0173 - val_loss: 0.0192\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0171 - val_loss: 0.0194\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0168 - val_loss: 0.0196\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0167 - val_loss: 0.0199\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0165 - val_loss: 0.0200\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0164 - val_loss: 0.0200\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0162 - val_loss: 0.0199\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0160 - val_loss: 0.0196\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0158 - val_loss: 0.0194\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0156 - val_loss: 0.0192\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0154 - val_loss: 0.0191\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0152 - val_loss: 0.0191\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0150 - val_loss: 0.0193\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0148 - val_loss: 0.0195\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0145 - val_loss: 0.0197\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0143 - val_loss: 0.0198\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0141 - val_loss: 0.0198\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0138 - val_loss: 0.0196\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0136 - val_loss: 0.0195\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0133 - val_loss: 0.0195\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0130 - val_loss: 0.0195\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0127 - val_loss: 0.0194\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0125 - val_loss: 0.0195\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0121 - val_loss: 0.0195\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0119 - val_loss: 0.0194\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0115 - val_loss: 0.0194\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0112 - val_loss: 0.0194\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0109 - val_loss: 0.0193\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0105 - val_loss: 0.0194\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0102 - val_loss: 0.0194\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0098 - val_loss: 0.0195\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0095 - val_loss: 0.0194\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0091 - val_loss: 0.0193\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0087 - val_loss: 0.0191\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0083 - val_loss: 0.0192\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0079 - val_loss: 0.0193\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0075 - val_loss: 0.0194\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0072 - val_loss: 0.0194\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0067 - val_loss: 0.0192\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0064 - val_loss: 0.0193\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0060 - val_loss: 0.0194\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0055 - val_loss: 0.0195\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5748\n",
            "Test Loss: 0.5747529864311218\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"e8a16b29-a17d-40f5-8dd9-7ab1ca54120a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e8a16b29-a17d-40f5-8dd9-7ab1ca54120a\")) {                    Plotly.newPlot(                        \"e8a16b29-a17d-40f5-8dd9-7ab1ca54120a\",                        [{\"mode\":\"lines\",\"name\":\"Actual GDP\",\"x\":[1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024],\"y\":[0.09433962264150952,0.16206896551724134,0.011869436201780343,-0.0029325513196479746,0.004411764705882275,0.02635431918008779,0.024251069900142756,0.013927576601671321,0.02884615384615372,0.02937249666221639,0.007782101167315218,0.024453024453024438,0.01758793969849244,0.03209876543209866,0.020334928229665157,0.01992966002344658,-0.008045977011494164,-0.010428736964078773,-0.022248243559718956,0.0035928143712574217,-0.007159904534606245,-0.01802884615384617,-0.008567931456548253,-0.007407407407407448,-0.007462686567164221,-0.007518796992481071,-0.010101010101010215,-0.014030612244898008,-0.010349288486416497,-0.14901960784313717,0.5668202764976956,-0.24411764705882347,-0.04928664072632941,0.09549795361527956,-0.18181818181818185],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Predicted unemp\",\"x\":[2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053],\"y\":[-0.18522909283638,0.05515545979142189,-0.08832169324159622,0.04220418632030487,0.18860319256782532,0.10342755168676376,0.16843566298484802,0.045950379222631454,0.03568674996495247,0.04832346737384796,0.025509335100650787,-0.03377867490053177,0.0061913831159472466,0.003042809898033738,0.015622856095433235,0.005897019058465958,0.017201535403728485,0.045727673918008804,0.027707424014806747,0.0219477117061615,0.01442289911210537,-0.008465537801384926,-0.002556998049840331,-0.029570281505584717,-0.034176018089056015,-0.039520442485809326,-0.04374765232205391,-0.03798385336995125,-0.039592280983924866,-0.038615915924310684],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Predicted unemployment of a Country for the Next 30 Years\"},\"xaxis\":{\"title\":{\"text\":\"Year\"}},\"yaxis\":{\"title\":{\"text\":\"unemp\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e8a16b29-a17d-40f5-8dd9-7ab1ca54120a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Scatter({\n",
              "    'mode': 'lines',\n",
              "    'name': 'Predicted unemp',\n",
              "    'x': [2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035,\n",
              "          2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047,\n",
              "          2048, 2049, 2050, 2051, 2052, 2053],\n",
              "    'y': array([-0.1852291 ,  0.05515546, -0.08832169,  0.04220419,  0.1886032 ,\n",
              "                 0.10342755,  0.16843566,  0.04595038,  0.03568675,  0.04832347,\n",
              "                 0.02550934, -0.03377867,  0.00619138,  0.00304281,  0.01562286,\n",
              "                 0.00589702,  0.01720154,  0.04572767,  0.02770742,  0.02194771,\n",
              "                 0.0144229 , -0.00846554, -0.002557  , -0.02957028, -0.03417602,\n",
              "                -0.03952044, -0.04374765, -0.03798385, -0.03959228, -0.03861592],\n",
              "               dtype=float32)\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "\n",
        "# Extracting the unemp values\n",
        "unemp_values = unemp['Unemp'].values.reshape(-1, 1)\n",
        "# Normalize unemp data\n",
        "scaler = MinMaxScaler(feature_range=(-1,1))  # Allow for negative values\n",
        "unemp_scaled = scaler.fit_transform(unemp_values)\n",
        "\n",
        "# Define the number of previous years to consider for prediction\n",
        "window_size = 15\n",
        "\n",
        "# Create sequences and labels\n",
        "sequences = []\n",
        "labels = []\n",
        "for i in range(len(unemp_scaled) - window_size):\n",
        "    sequences.append(unemp_scaled[i:i + window_size])\n",
        "    labels.append(unemp_scaled[i + window_size])\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape sequences for CNN input (samples, window_size, 1)\n",
        "X_train = X_train.reshape(X_train.shape[0], window_size, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], window_size, 1)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, input_shape=(window_size, 1)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=64, kernel_size=3),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(units=128),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dense(units=64),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dense(units=1, activation='linear')  # Use linear activation for flexibility\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# Predict unemp for the next 10 years\n",
        "last_sequence = sequences[-1].reshape(1, window_size, 1)\n",
        "future_unemp_scaled = []\n",
        "\n",
        "# Use the model to predict unemp\n",
        "for i in range(30):\n",
        "    prediction = model.predict(last_sequence)\n",
        "    future_unemp_scaled.append(prediction)\n",
        "    last_sequence = np.concatenate([last_sequence[:, 1:, :], prediction.reshape(1, 1, 1)], axis=1)\n",
        "\n",
        "future_unemp_scaled = np.array(future_unemp_scaled).reshape(-1)\n",
        "predicted_unemp = scaler.inverse_transform(future_unemp_scaled.reshape(-1, 1))\n",
        "\n",
        "\n",
        "# Plotting the results using Plotly\n",
        "years = unemp['Year']\n",
        "actual_trace = go.Scatter(x=years, y=unemp_values.flatten(), mode='lines', name='Actual GDP')\n",
        "predicted_trace = go.Scatter(x=list(range(max(years), max(years) + 30)), y=predicted_unemp.flatten(), mode='lines', name='Predicted unemp')\n",
        "\n",
        "layout = go.Layout(title='Predicted unemployment of a Country for the Next 30 Years',\n",
        "                   xaxis=dict(title='Year'),\n",
        "                   yaxis=dict(title='unemp'))\n",
        "\n",
        "fig = go.Figure(data=[actual_trace, predicted_trace], layout=layout)\n",
        "fig.show()\n",
        "predicted_trace"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}